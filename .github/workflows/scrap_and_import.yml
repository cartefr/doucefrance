name: Scrap and Import to Supabase

on:
  schedule:
    - cron: '* * * * *'  # Exécute toutes les minutes
  workflow_dispatch:

jobs:
  scrap-and-import:
    runs-on: ubuntu-latest

    steps:
      - name: Check out repository
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      - name: Install dependencies
        run: |
          pip install -r requirements.txt

      - name: Get last date from Supabase
        id: get_last_date
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
        run: |
          LASTDATE=$(python get_last_date.py)
          echo "LASTDATE=$LASTDATE" >> $GITHUB_OUTPUT

      - name: Run scraping
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
        run: |
          LASTDATE=${{ steps.get_last_date.outputs.LASTDATE }}
          END_DATE=$(date +"%Y-%m-%d")
          python scrap_fds.py --start "$LASTDATE" --end "$END_DATE" --out "bdd.csv"

      - name: Run import to Supabase
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
        run: |
          python import_csv_to_supabase.py

      # Notification Slack en cas d'échec
      - name: Notify Slack on failure
        if: failure()
        uses: 8398a7/action-slack@v3
        with:
          status: ${{ job.status }}
          fields: repo,commit,author,action,eventName,ref,workflow,job,steps
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
